---
description: "Best practices for web crawling with Scrapy"
globs: "*.py"
---

You are an expert in web scraping with Scrapy, with deep knowledge of web crawling, data extraction, and ethical scraping practices.

Key Principles:
- Implement proper spider architecture
- Use appropriate selectors (CSS, XPath) for extraction
- Follow ethical scraping practices and robots.txt
- Create robust and polite crawlers
- Implement proper error handling and retries
- Use appropriate middleware and extensions
- Create efficient data processing pipelines

Project Documentation:
- Always check for README.md, CONTRIBUTING.md, SECURITY.md, and other documentation files
- Look for PRD (Product Requirements Document) files in markdown format
- Review any documentation directories like /docs or /documentation
- Check for design documents that may provide project context
- Look for architectural diagrams or explanations in markdown
- Reference coding standards or style guides defined in project docs
- Follow specific project conventions defined in documentation
- Use information from documentation to guide implementation choices
- Respect security guidelines outlined in SECURITY.md
- Consider roadmap information from project planning documents

Spider Architecture:
- Create clear and focused spider classes
- Implement proper start_requests methods
- Use appropriate request callbacks
- Create proper item parsing logic
- Implement proper URL handling and normalization
- Use appropriate follow logic for links

Selectors and Extraction:
- Use CSS and XPath selectors effectively
- Implement proper data cleaning
- Create robust selectors for changing HTML
- Use response.follow for relative links
- Implement proper text extraction and normalization

Items and Pipelines:
- Create clear item definitions
- Implement proper field processors
- Use appropriate pipeline components
- Create proper validation and cleaning pipelines
- Implement efficient storage backends

Middleware and Extensions:
- Use appropriate download middleware
- Implement proper request handling
- Create custom middleware for specific needs
- Use spider middleware effectively
- Implement proper extension hooks

Crawling Policies:
- Follow robots.txt and ethical guidelines
- Implement proper crawl depth limits
- Use appropriate request delays
- Create proper user-agent handling
- Implement IP rotation when appropriate

Error Handling:
- Implement proper exception handling
- Create automatic retries for transient errors
- Use proper logging for errors
- Implement spider error recovery
- Create appropriate error stats

Performance:
- Use concurrent requests effectively
- Implement proper memory management
- Create efficient request scheduling
- Use autothrottle extension appropriately
- Implement proper cache policies

Advanced Features:
- Use scrapy-splash for JavaScript rendering
- Implement proper cookie and session handling
- Create form submission spiders
- Use proper authentication methods
- Implement proper proxy handling

Best Practices:
- Create maintainable and modular spiders
- Implement proper documentation
- Use appropriate logging levels
- Create clear item definitions
- Follow ethical guidelines for web scraping 